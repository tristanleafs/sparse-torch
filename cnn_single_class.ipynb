{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Modules you need for this lab\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "\n",
    "# Other non-PyTorch Modules\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "def show_data(data_sample, size):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(size, size), cmap='gray')\n",
    "    plt.title(f\"Real Label = {labels[data_sample[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSparse(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # image, label = sample['image'], sample['label']\n",
    "\n",
    "        temp = (torch.rand(size=sample.shape) < 0.5).float()\n",
    "        return sample*temp\n",
    "\n",
    "        # return {'image': torch.multiply(image,temp), 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleClass(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, sample):\n",
    "\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 24\n",
    "#Generates an object to store multiple transformations\n",
    "\n",
    "composed = transforms.Compose(\n",
    "    [transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    SingleClass()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the two dataset objects and applying our transformations from above\n",
    "\n",
    "dataset_train = dsets.FashionMNIST(root= '.fashion/data', train=True, transform=composed,  download=True)\n",
    "  \n",
    "dataset_val = dsets.FashionMNIST(root= '.fashion/data', train=False, transform=composed,  download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 9\n",
    "dataset_train.targets[dataset_train.targets != num] = 1\n",
    "dataset_train.targets[dataset_train.targets == num] = 0\n",
    "dataset_val.targets[dataset_val.targets != num] = 1\n",
    "dataset_val.targets[dataset_val.targets == num] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n,data_sample in enumerate(dataset_val):\n",
    "\n",
    "    show_data(data_sample, IMAGE_SIZE)\n",
    "    plt.show()\n",
    "    if n==1:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every PyTorch model must inherit from torch.nn.Module\n",
    "#nn.Module has very useful functions for models\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Constructor: (out_1: Output channels of first CNN Layer), (out_2: Output channels of second CNN Layer), (number_of_classes: Number of classes to detect)\n",
    "    def __init__(self, out_1=16, out_2=64, number_of_classes=10):\n",
    "        super().__init__()\n",
    "        #Create first Convolution Layer with input of 1 channel (grayscale) and output of out_1 channels\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        #Create a fully connected layer for the CNN. The input shape is the flattened convolution output. If output is (3, 28, 28), input is 28 * 28 * 3.\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)\n",
    "\n",
    "        # Prediction (x is input)\n",
    "\t# The forward function is automatically called when we create an instance of the class and call it.\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "     \n",
    "        \n",
    "        #Flattening cnn2's output and passing it into a fully connected layer\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader for training data\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=100 )\n",
    "\n",
    "#Data Loader for validation data\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 10, 3),\n",
    "    nn.MaxPool2d(3,3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(490, 2)\n",
    ")\n",
    "\n",
    "#Creating an Adam optimizer with a learning rate of 0.002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.002)\n",
    "\n",
    "#Use Cross Entropy Loss since this is a classification problem\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch no. 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:10<00:00, 58.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9739666666666666\n",
      "Validation accuracy: 0.9845\n",
      "\n",
      "Epoch no. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:10<00:00, 58.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9872\n",
      "Validation accuracy: 0.9883\n",
      "\n",
      "Epoch no. 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:10<00:00, 58.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.98935\n",
      "Validation accuracy: 0.9893\n",
      "\n",
      "Epoch no. 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:09<00:00, 60.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9903833333333333\n",
      "Validation accuracy: 0.9903\n",
      "\n",
      "Epoch no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:10<00:00, 59.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9907166666666667\n",
      "Validation accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "# To show a progress bar while training\n",
    "from tqdm import tqdm\n",
    "\n",
    "training_acc_list=[]\n",
    "val_acc_list=[]\n",
    "N_test=len(dataset_val)\n",
    "N_train = len(dataset_train)\n",
    "n_epochs=5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #Count how many predictions were correct\n",
    "    correct = 0\n",
    "\n",
    "    print()\n",
    "    print(f\"Epoch no. {epoch}\")\n",
    "    #Put model into training mode (uses Dropout if there are Dropout layers)\n",
    "    model.train()\n",
    "    #Loop over each sample in the dataloaders.\n",
    "    for x, y in tqdm(train_loader):\n",
    "        #Zero the optimizer gradients (PyTorch requires this.)\n",
    "        optimizer.zero_grad()\n",
    "        #Make a prediction\n",
    "        y_hat = model(x)\n",
    "        #Calculate the loss with the criterion\n",
    "        loss = criterion(y_hat, y)\n",
    "        #Initiate backpropagation/calculate derivatives of parameters with respect to the loss.\n",
    "        loss.backward()\n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Update correct counter\n",
    "        #Grab the index of the AI's highest probability guess (Each output of model(x) is a tensor of probabilities) \n",
    "        correct += (torch.argmax(y_hat, 1) == y).sum().item()\n",
    "        \n",
    "    #Calculate accuracy\n",
    "    accuracy = correct / N_train\n",
    "    print(\"Training accuracy: \" + str(accuracy))\n",
    "    training_acc_list.append(accuracy)\n",
    "\n",
    "    correct=0\n",
    "    \n",
    "    #Put model into validation mode (turns off Dropout if there are Dropout layers)\n",
    "    model.eval()\n",
    "    for x_test, y_test in test_loader:\n",
    "        #Make a prediction\n",
    "        z = model(x_test)\n",
    "        #Grab the index of the AI's highest probability guess (Each output of model(x) is a tensor of probabilities) \n",
    "        y_hat = torch.argmax(z, 1)\n",
    "        #Update correct counter if the prediction was correct\n",
    "        correct += (y_hat == y_test).sum().item()\n",
    "    accuracy = correct / N_test\n",
    "    print(\"Validation accuracy: \" + str(accuracy))\n",
    "    val_acc_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sequential(\\n  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\\n  (1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\\n  (2): ReLU()\\n  (3): Flatten(start_dim=1, end_dim=-1)\\n  (4): Linear(in_features=490, out_features=2, bias=True)\\n)'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'models/model_9.pth')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c74322094aab99573c56dbb21d7e1901a2c3b2c7428609116d3e4a163cd557a7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
