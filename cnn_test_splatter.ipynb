{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trist\\anaconda3\\envs\\env_torch\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\trist\\anaconda3\\envs\\env_torch\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Modules you need for this lab\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "\n",
    "# Other non-PyTorch Modules\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "def show_data(data_sample, size):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(size, size), cmap='gray')\n",
    "    plt.title(f\"Real Label = {labels[data_sample[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 24\n",
    "#Generates an object to store multiple transformations\n",
    "\n",
    "composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the two dataset objects and applying our transformations from above\n",
    "\n",
    "dataset_train = dsets.FashionMNIST(root= '.fashion/data', train=True, transform=composed,  download=True)\n",
    "  \n",
    "dataset_val = dsets.FashionMNIST(root= '.fashion/data', train=False, transform=composed,  download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUdklEQVR4nO3de4xc5X3G8e+Dwbe10dpADTEGA3YBi4KxXC4FESJCuF+iqlFMGtEE1QSBAlJK6tIo0BbSRGoSIjWK6ihc2gJpFEhiqaQB0bhcQpMYsuVasGMcYWNjG8e3+IbtX/+Y42Zwdv2+3jmzM973+UijnT3zmzm/OTPPnrm8+x5FBGY2/B3U6QbMbGg47GaFcNjNCuGwmxXCYTcrhMNuVgiHvYMknS9peRtud5mkDw71deu29/bppt4ORA57hupJtlXSZkmrJN0nadwQrDckTWv3etpN0kJJ26rtt1bSI5KO6nRfpXHY810REeOAmcDpwF91tp0Dzk3V9vt9oBf4amfbSVPDsMnIsLkjQyUiVgE/ohF6ACSdJeknktZL+h9J5zdd9glJr0raJGmppOtb7UHSCZL+U9I71Z7yAUm9e5X9oaRXJP1a0r2SRjdd/3JJfVW/P5F0aqs95YqIdcDDwClVL+959VK9arozdTuSRkm6W9Jb1eluSaOqy16VdHlT7cGS1kiaVf2+r8droaS7JD0DbAGOr+eed57Dvp8kHQ1cAiypfp8M/DtwJzAR+AvgYUlHVFdZDVwOHAp8AvjqniddK20Afw+8DzgZmALcsVfNx4CLgBNo7E0/V/V7OnAPcD1wGPBPwII9QdnnSqV5VUD6PWU1Lh0O/DHwi5z6ffhr4Cwaf3RPA86guo/AQ8CcptqLgLUR8XzG4wXwcWAuMB74VYt9do+I8ClxApYBm4FNQABPAL3VZX8J/Mte9T8Crh3gtr4P3FydPx9Yvo/1BjAto7+rgV/s1e+nmn6/FPhldf4bwN/tdf3XgPc3XfeDNW+/hTT2kuuBFcADwBH93UfgPuDO/rZPc2/AL4FLmy67CFhWnZ9WPVZjq98fAD6f83hVvf5tp59z7Th5z57v6ogYT+MJeBJweLX8WOBP9trDnQscBSDpEkn/LWldddmlTdcdFEmTJH1b0gpJG4F/7ec232w6/ysarwL29PuZvfqd0nR5u3w6InojYnJEfCwi1rR4e+/jvXvd/7+PEbEEeBW4QtJY4Ergwapun49XpXnbDRsO+36KiP+isff5h2rRmzT2FL1Np56I+GL10vjhqnZSRPQCj9J4Gd6KL9DYI/5BRBwK/Gk/tzml6fwxwFtN/d61V79jI+Kh1Eol3VZ9ot7vaZD3ZQswtun3IzOv9xaN4O7RfB/hty/lrwJeqf4AwD4er6brDst/BXXYB+du4EJJp9HYq14h6SJJIySNrr4fPhoYCYwC1gA7JV0CfGg/1zWyus09pxE03ktuBjZU70Fv7ed6N0o6WtJEGu9v/61a/k3gU5LOrD5t7pF0maTxqUYi4gsRMW6g037erz36gGuqbXcx8P7M6z0EfE7SEdXnAJ+n8Vjs8W0a2/oGfrtXh30/XsOawz4I1UvQf6bxPvBNGnuP22iE+k0a4TsoIjYBnwa+A/wauAZYsJ+rexnY2nT6BPA3wCxgA40Pmx7p53oPAo8BS2m8v72z6n0R8OfAP1Y9LQH+bD97qtPNwBU03s9/jMZnGjnuBBYBLwAvAs9XywCIiJXAs8Af8ds/dOzr8WrpXhwAVH0oYWbD3LD/a2ZmDQ67WSEcdrNCOOxmhTh4KFcmaVh+GijlfW1+8MHpzT1ixIhkzejRo5M1O3fuTNbs2rUrWZMjp+ec+/7uu+8ma3Lu11De924UEf0+IYc07MNVzhMZYNKkScma3t7eZM306dOTNe+8806yZsOGDcmanD9k48alv2LPue8rV65M1qxevTpZs3bt2mRNzn0HGE7fVrX0Ml7SxZJek7RE0ry6mjKz+g067NVIrq/T+A+wGcAcSTPqaszM6tXKnv0MYElELI2IHTSGJ15VT1tmVrdWwj6Z9/530PJq2XtImitpkaRFLazLzFrU9g/oImI+MB+G76fxZgeCVvbsK3jvv1EeXS0zsy7USth/DkyXdJykkcBH2f//6DKzITLol/ERsVPSTTSm9BkB3BMRL9fWWZeYNi09k/PZZ5+ddVtz5sxJ1pxyyinJmvHjk/96zkEHpf+O9/T0JGtyBszkDGLZunVrsiZnoMvy5elp9n/4wx8ma7773e8mawB+9rOfZdUdCFp6zx4Rj9KYecXMupzHxpsVwmE3K4TDblYIh92sEA67WSEcdrNCOOxmhRjSqaS7bWz8sccem6z50pe+lKy58MILs9Y3cuTIZM2OHTuSNTmDT3IG1eRMupEzeUVdM8PUNeNNjpdfzhv/dcsttyRrum3gzUAz1XjPblYIh92sEA67WSEcdrNCOOxmhXDYzQrhsJsVwmE3K0TRR4S55pprkjU5A2ZyDyWUM2BmzJgxyZqcQSzbt29P1mzbti1ZkzMQKGfgTc7grboO/5TTz4knnpisAbjuuuuSNX19fcmanMe+3bxnNyuEw25WCIfdrBAOu1khHHazQjjsZoVw2M0K4bCbFaLoQTWnn356sqau2WUgb6BLzmGJpk+fnqyZOXNmsmbs2LHJmpxBLFu2bEnWHHLIIcmaUaNGJWtyBt7kHGoqZ1YcgOOOOy5ZM3HixGTNqlWrstbXTt6zmxXCYTcrhMNuVgiH3awQDrtZIRx2s0I47GaFcNjNClH0oJqcATM5h1HKHaCxePHiZM1dd92VrLn11luTNWeeeWay5rnnnkvWfPazn03WnHzyycmaO+64I1lz7733JmveeuutZE1Oz7mHkZo0aVKyZtq0ackaD6oxsyHT0p5d0jJgE7AL2BkRs+toyszqV8fL+A9ExNoabsfM2sgv480K0WrYA3hM0nOS5vZXIGmupEWSFrW4LjNrQasv48+NiBWSfg94XNL/RsSTzQURMR+YDyApPXm4mbVFS3v2iFhR/VwNfA84o46mzKx+gw67pB5J4/ecBz4EvFRXY2ZWr1Zexk8Cvlcdaudg4MGI+I9auqpBzswoOYNhdu/eXcu6IO+QQxdccEEtPeXM1pIzOOeUU05J1qxbty5Zk3OorZyeN2/enKy54YYbkjU9PT3JGoAJEyYka44//vhkzdNPP521vnYadNgjYilwWo29mFkb+as3s0I47GaFcNjNCuGwmxXCYTcrhMNuVgiH3awQDrtZIYbttFQ5I59yjnWWI3cEXc6orSOPPLLVdgD42te+lqx5/fXXkzU5Uy7lHOtszZo1yZqcx2PKlCnJmtwpp3KMGzcuWZMzdVU38J7drBAOu1khHHazQjjsZoVw2M0K4bCbFcJhNyuEw25WiGE7qObQQw9N1tQ1qKaamispZ7BHzmCghQsXJmvWr1+frJk5c2ayJmfKpVNPPTVZc9555yVr+vr6kjWzZs1K1uQ89rlyBkzlPGbdwHt2s0I47GaFcNjNCuGwmxXCYTcrhMNuVgiH3awQDrtZIYbtoJre3t5kzejRo2tZV+6gmhw5M8PkHDds9uzZyZorr7wyWfPoo48ma3KOv5bTz4IFC5I1u3btStZcfvnlyZrcxyzneIB1Dc5qN+/ZzQrhsJsVwmE3K4TDblYIh92sEA67WSEcdrNCOOxmhSh6UM2YMWOSNRGRrDnooLy/mTm3dfbZZydrTjvttGRNzn3LGQxy4oknJmtyZuDJmfHl9ttvT9aMHDkyWZMzU822bduSNblyHv+cQTw5z49WJLuUdI+k1ZJealo2UdLjkhZXPw+MeXnMCpazS7oPuHivZfOAJyJiOvBE9buZdbFk2CPiSWDdXouvAu6vzt8PXF1vW2ZWt8G+Z58UESur86uAAY9ZK2kuMHeQ6zGzmrT8AV1EhKQBP1mIiPnAfIB91ZlZew32q7e3JR0FUP1cXV9LZtYOgw37AuDa6vy1wA/qacfM2iXnq7eHgGeBEyUtl3Qd8EXgQkmLgQ9Wv5tZF0u+Z4+IOQNcdEHNvdRq/PjxyZpRo0Yla3IGOmzcuDGrpzfeeCNZk3O4pZzZU3Ls3LkzWZMzGGT37t3Jmu3btydrpkyZkqzJ6Xnp0qXJmqlTpyZrAHbs2JGsyXmujRs3LlmzadOmrJ4Gy8NlzQrhsJsVwmE3K4TDblYIh92sEA67WSEcdrNCOOxmhRi2M9XUNagm5xBRfX19OS3xzDPPJGtOOumkZE3OQI+cwTA5g3NyBszkqGvGn5yaxx57LFnzyU9+MlkDefc/Z8afnp6eZI0H1ZhZLRx2s0I47GaFcNjNCuGwmxXCYTcrhMNuVgiH3awQw3ZQTc4ghroOW7R27dqsnnJmWckZxJFTkzNgpt2HG2qWM8gnR87hnxYvXpysyXksIG8b5dTkPNfazXt2s0I47GaFcNjNCuGwmxXCYTcrhMNuVgiH3awQDrtZITr/TX+b5MwwkyNnAMtvfvObrNs65phjallfziCOnEEsQzmoJkfOfc8ZLJVzqKlcOdsxp6ec2WzazXt2s0I47GaFcNjNCuGwmxXCYTcrhMNuVgiH3awQDrtZIYbtoJqcwwTlyJnRZMWKFVm3NWvWrGRNXYdAyhkMMpQDb3LWVdcsPWvWrEnWbN68OVkDMGHChGTNmDFjkjV1DfJqhffsZoVIhl3SPZJWS3qpadkdklZI6qtOl7a3TTNrVc6e/T7g4n6WfzUiZlanR+tty8zqlgx7RDwJrBuCXsysjVp5z36TpBeql/kDfoohaa6kRZIWtbAuM2vRYMP+DeAEYCawEvjyQIURMT8iZkfE7EGuy8xqMKiwR8TbEbErInYD3wTOqLctM6vboMIu6aimXz8MvDRQrZl1h+SgGkkPAecDh0taDtwOnC9pJhDAMuD69rU4ODmDL3bt2pWsyRlUkjszyuTJk7PqUuo6lFJd6uqnrll6cgaw5B6y6/DDD0/W1HUYsXZLdhkRc/pZ/K029GJmbeQRdGaFcNjNCuGwmxXCYTcrhMNuVgiH3awQDrtZIYbtTDU5M8zUdRil3EP79Pb2Jmty+q5rFp66dNuMN+ecc06yZtWqVVnrmzFjRrJm1KhRyRrPVGNmQ8ZhNyuEw25WCIfdrBAOu1khHHazQjjsZoVw2M0KMWwH1eTMepIzw8iIESOSNVOnTs1pKWt9OYNq6poZpq6BLjm3U9cAppzbmTZtWrJm2bJlyRrI6ynnOdINM9V4z25WCIfdrBAOu1khHHazQjjsZoVw2M0K4bCbFcJhNyuEw25WiGE7gi5nJFpdx3rbsGFDVk85I61yRv7Vpa6Rb3VNk5VzOzt27EjWHHnkkcmavr6+nJay5DyuOaMn2817drNCOOxmhXDYzQrhsJsVwmE3K4TDblYIh92sEA67WSE6/01/m+RMJ/Tuu+8ma7Zt25aseeqpp7J6uuyyy5I1OdMXbd26NWt9KXVNS1XXQKCcfnp6epI1S5cuTdY8//zztfU0cuTIZM0Bcaw3SVMk/VjSK5JelnRztXyipMclLa5+Tmh/u2Y2WDkv43cCn4mIGcBZwI2SZgDzgCciYjrwRPW7mXWpZNgjYmVEPF+d3wS8CkwGrgLur8ruB65uU49mVoP9es8uaSpwOvBTYFJErKwuWgVMGuA6c4G5LfRoZjXI/jRe0jjgYeCWiNjYfFk0PsXo95OMiJgfEbMjYnZLnZpZS7LCLukQGkF/ICIeqRa/Lemo6vKjgNXtadHM6pDzabyAbwGvRsRXmi5aAFxbnb8W+EH97ZlZXXLes58DfBx4UVJftew24IvAdyRdB/wK+EhbOjSzWiTDHhFPAwONULmg3nbqs379+mTN9u3bkzU5M6O8/vrrOS0xb17628mcWU9yBvrUpa5ZaHLkDM4ZO3ZssiZnUE2unBmPcmpynkft5uGyZoVw2M0K4bCbFcJhNyuEw25WCIfdrBAOu1khHHazQgzbmWqWLFmSrNm4cWOy5rDDDkvWLFu2LKclFi5cmFVnrckZCHTRRRdl3VbOIKecwVk5z7V2857drBAOu1khHHazQjjsZoVw2M0K4bCbFcJhNyuEw25WiGE7qGbNmjXJmtdeey1ZkzPDyJYtW7J6yjkkVU6N7VvOIZveeOONrNt69tlnkzUvvPBCsmb16s7Px+o9u1khHHazQjjsZoVw2M0K4bCbFcJhNyuEw25WCIfdrBDKGYBQ28qkNTSOC7fH4cDaIWugPgdi3+556HSy72Mj4oj+LhjSsP/OyqVFB+Jx2w/Evt3z0OnWvv0y3qwQDrtZITod9vkdXv9gHYh9u+eh05V9d/Q9u5kNnU7v2c1siDjsZoXoWNglXSzpNUlLJM3rVB/7Q9IySS9K6pO0qNP9DETSPZJWS3qpadlESY9LWlz9nNDJHvc2QM93SFpRbe8+SZd2sse9SZoi6ceSXpH0sqSbq+Vdua07EnZJI4CvA5cAM4A5kmZ0opdB+EBEzOzG71Gb3AdcvNeyecATETEdeKL6vZvcx+/2DPDVanvPjIhHh7inlJ3AZyJiBnAWcGP1PO7Kbd2pPfsZwJKIWBoRO4BvA1d1qJdhJyKeBNbttfgq4P7q/P3A1UPZU8oAPXe1iFgZEc9X5zcBrwKT6dJt3amwTwbebPp9ebWs2wXwmKTnJM3tdDP7aVJErKzOrwImdbKZ/XCTpBeql/ld8XK4P5KmAqcDP6VLt7U/oNs/50bELBpvP26UdF6nGxqMaHzfeiB85/oN4ARgJrAS+HJHuxmApHHAw8AtEfGew7V207buVNhXAFOafj+6WtbVImJF9XM18D0ab0cOFG9LOgqg+tn56U4TIuLtiNgVEbuBb9KF21vSITSC/kBEPFIt7spt3amw/xyYLuk4SSOBjwILOtRLFkk9ksbvOQ98CHhp39fqKguAa6vz1wI/6GAvWfYEpvJhumx7qzHv97eAVyPiK00XdeW27tgIuuprlLuBEcA9EXFXRxrJJOl4GntzaMy3/2C39izpIeB8Gv9q+TZwO/B94DvAMTT+zfgjEdE1H4gN0PP5NF7CB7AMuL7pvXDHSToXeAp4EdhdLb6Nxvv2rtvWHi5rVgh/QGdWCIfdrBAOu1khHHazQjjsZoVw2M0K4bCbFeL/ANl+CU2G5LzqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n,data_sample in enumerate(dataset_val):\n",
    "\n",
    "    show_data(data_sample, IMAGE_SIZE)\n",
    "    # print(data_sample)\n",
    "    plt.show()\n",
    "    if n==1:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every PyTorch model must inherit from torch.nn.Module\n",
    "#nn.Module has very useful functions for models\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Constructor: (out_1: Output channels of first CNN Layer), (out_2: Output channels of second CNN Layer), (number_of_classes: Number of classes to detect)\n",
    "    def __init__(self, out_1=16, out_2=64, number_of_classes=10):\n",
    "        super().__init__()\n",
    "        #Create first Convolution Layer with input of 1 channel (grayscale) and output of out_1 channels\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        #Create a fully connected layer for the CNN. The input shape is the flattened convolution output. If output is (3, 28, 28), input is 28 * 28 * 3.\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)\n",
    "\n",
    "        # Prediction (x is input)\n",
    "\t# The forward function is automatically called when we create an instance of the class and call it.\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "     \n",
    "        \n",
    "        #Flattening cnn2's output and passing it into a fully connected layer\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader for training data\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=100 )\n",
    "\n",
    "#Data Loader for validation data\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_classes import Splatter\n",
    "\n",
    "#Creating the model\n",
    "model = nn.Sequential(\n",
    "    Splatter(3, 3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(484, 10)\n",
    ")\n",
    "\n",
    "#Creating an Adam optimizer with a learning rate of 0.002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.002)\n",
    "\n",
    "#Use Cross Entropy Loss since this is a classification problem\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch no. 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 13/600 [00:52<39:31,  4.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13792/639018670.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#Make a prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m#Calculate the loss with the criterion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\trist\\Documents\\Dev\\torch_start\\custom_classes.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSplatter_Conv2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\trist\\Documents\\Dev\\torch_start\\custom_classes.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, input, filter, bias)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# result = splatter_forward_full(input.numpy(), filter.numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplatter_cpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# result = splat_corr2d(input.numpy(), filter.numpy(), mode=\"valid\") # supposed to be correlate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# To show a progress bar while training\n",
    "from tqdm import tqdm\n",
    "\n",
    "training_acc_list=[]\n",
    "val_acc_list=[]\n",
    "N_test=len(dataset_val)\n",
    "N_train = len(dataset_train)\n",
    "n_epochs=5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #Count how many predictions were correct\n",
    "    correct = 0\n",
    "\n",
    "    print()\n",
    "    print(f\"Epoch no. {epoch}\")\n",
    "    #Put model into training mode (uses Dropout if there are Dropout layers)\n",
    "    model.train()\n",
    "    #Loop over each sample in the dataloaders.\n",
    "    for x, y in tqdm(train_loader):\n",
    "        #Zero the optimizer gradients (PyTorch requires this.)\n",
    "        optimizer.zero_grad()\n",
    "        #Make a prediction\n",
    "        y_hat = model(x)\n",
    "        #Calculate the loss with the criterion\n",
    "        loss = criterion(y_hat, y)\n",
    "        #Initiate backpropagation/calculate derivatives of parameters with respect to the loss.\n",
    "        loss.backward()\n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Update correct counter\n",
    "        #Grab the index of the AI's highest probability guess (Each output of model(x) is a tensor of probabilities) \n",
    "        correct += (torch.argmax(y_hat, 1) == y).sum().item()\n",
    "        \n",
    "    #Calculate accuracy\n",
    "    accuracy = correct / N_train\n",
    "    print(\"Training accuracy: \" + str(accuracy))\n",
    "    training_acc_list.append(accuracy)\n",
    "\n",
    "    correct=0\n",
    "    \n",
    "    #Put model into validation mode (turns off Dropout if there are Dropout layers)\n",
    "    model.eval()\n",
    "    for x_test, y_test in test_loader:\n",
    "        #Make a prediction\n",
    "        z = model(x_test)\n",
    "        #Grab the index of the AI's highest probability guess (Each output of model(x) is a tensor of probabilities) \n",
    "        y_hat = torch.argmax(z, 1)\n",
    "        #Update correct counter if the prediction was correct\n",
    "        correct += (y_hat == y_test).sum().item()\n",
    "    accuracy = correct / N_test\n",
    "    print(\"Validation accuracy: \" + str(accuracy))\n",
    "    val_acc_list.append(accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c74322094aab99573c56dbb21d7e1901a2c3b2c7428609116d3e4a163cd557a7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
